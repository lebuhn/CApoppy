---
title: "Climate_data"
format: html
editor: visual
---

This file imports and aggregates data files for PAR and rh/temp from 4 sites: Percos, Pt. Conception, Perry Field and Cojo HQ.

```{r setup}


# We'll use this later for data visualization tasks
library(ggplot2)

#
# dplyr provides easy-to-read functions for data frames
library(dplyr)


# readr gives faster and more consistent reading than base R
library(readr)


# stringr helps with renaming and parsing file names or column names
library(stringr)


# tidyr is useful for cleaning messy datasets
library(tidyr)

library(lubridate)

```

The data are collected on an Apogee PAR sensor and a relative humidity and temperature sensor deployed at each site. These data were gathered at the start of the flowering season.

NB:The Percos rh/temp sensor did not gather data correctly but when tested in the lab, did fine.

Set directories and list files for import

```{r, import}

# Define the folder where your raw CSV files are stored
data_dir <- "../data/raw/rh_temp"

# List any files you want to ignore (e.g., test files)
exclude_files <- c("LeBuhn3_test_PC.csv")

# Create a list of all CSV files in the folder
# full.names = TRUE gives the full path for each file
file_list <- list.files(path = data_dir, pattern = "*.csv", full.names = TRUE)

# Remove the excluded files from the list
# This ensures only the needed files are kept
file_list <- file_list[!basename(file_list) %in% exclude_files]

```

Create a function to process files

```{r}

# Define a function that takes a file path and processes the file
# This helps us reuse the same cleaning steps for each file
process_file <- function(filepath) {

  # Get just the filename (without folder path)
  # This will be used to create a 'site name'
  filename <- basename(filepath)

  # Extract the site name from the filename using underscore as a separator
  # str_split returns a matrix when simplify = TRUE
  site_name <- str_split(filename, "_", simplify = TRUE)[1]

  # Read the CSV file into a dataframe
  # show_col_types = FALSE hides extra print messages
  df <- read_csv(filepath, show_col_types = FALSE)
  
 
  # Rename the first column to "Sample"
  # This helps us use consistent column names
  names(df)[1] <- "Sample"
  
 # Standardize column names 
colnames(df) <- str_replace_all(colnames(df), "Celsius.*C\\)", "Temp_C")
colnames(df) <- str_replace_all(colnames(df), "Dew Point.*C\\)", "Dew_Point_C")
colnames(df) <- str_replace_all(colnames(df), "Humidity.*rh\\)", "Humidity_RH")
colnames(df) <- str_replace_all(colnames(df), "Serial Number", "Serial_Number")


  # Remove unnamed or extra columns like "...8", "...9"
  # These often come from Excel exports or formatting errors
  df <- df %>% select(-matches("^\\.\\.\\."))

  # If the Serial_Number column is missing, create it with missing values
  # NA_character_ ensures the column has the correct type (character)
  if (!"Serial_Number" %in% colnames(df)) {
    df$Serial_Number <- NA_character_
  }

  # Convert Serial_Number to character and fill in missing values
  # First non-missing value is used to replace all NAs
  df <- df %>%
    mutate(Serial_Number = as.character(Serial_Number),
           Serial_Number = replace_na(Serial_Number, first(na.omit(Serial_Number))))

  # Add a new column called 'Site' with the extracted site name
  # This helps track where each row of data came from
  df <- df %>%
    mutate(Site = site_name)
  
  # Convert Time column from character to POSIXct date-time
# This format matches: "month/day/year hour:minute:second"
df <- df %>%
  mutate(Time = mdy_hms(Time))

  # Return the cleaned and updated data frame
  # The output will be used in the next step
  return(df)
}

```

Process all files

```{r}
# Use lapply to apply the process_file function to every file
# bind_rows() combines the list of data frames into one big data frame
rh_temp_data <- lapply(file_list, process_file) %>% bind_rows()

# Define output file path
output_file <- "../data/processed/RH_Temp_data.csv"

# Write the combined data to CSV
write.csv(rh_temp_data, file = output_file, row.names = FALSE)

```

Now we import PAR data

```{r}


# Define the folder where your raw CSV files are stored
data_dir <- "../data/raw/Apogee_PAR"

# Get list of CSV files in the directory
csv_files <- list.files(path = data_dir, pattern = "\\.csv$", full.names = TRUE)

# Read and combine all CSVs with site column
PAR_data <- lapply(csv_files, function(file) {
  # Extract site name from filename (before the first underscore)
  filename <- basename(file)
  site_name <- strsplit(filename, "_")[[1]][1]
  
  # Read the CSV and add site column
  df <- read.csv(file)
  df$site <- site_name
  return(df)
}) %>% bind_rows()

# Define output file path
output_file <- "../data/processed/PAR_data.csv"

# Write the combined data to CSV
write.csv(PAR_data, file = output_file, row.names = FALSE)


```
